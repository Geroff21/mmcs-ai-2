{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-3. Рекомендательные системы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Выбираем девайс\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать MovieLens\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "А именно, самый маленький вариант со 100 тыс. оценок\n",
    "\n",
    "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До добавления нового пользователя:\n",
      "1 610\n",
      "0 9741\n",
      "       userId  movieId  rating   timestamp\n",
      "0           1     1319     4.0   964981230\n",
      "1           1      801     5.0   964982400\n",
      "2           1     2608     4.0   964981775\n",
      "3           1      981     5.0   964982703\n",
      "4           1      275     3.0   964982310\n",
      "...       ...      ...     ...         ...\n",
      "80664     610     8686     4.0  1479542606\n",
      "80665     610     9389     3.5  1493848789\n",
      "80666     610     2970     2.0  1493849562\n",
      "80667     610     2916     4.5  1493847010\n",
      "80668     610     5640     5.0  1479545132\n",
      "\n",
      "[80669 rows x 4 columns]\n",
      "После добавления нового пользователя:\n",
      "1 611\n",
      "0 9741\n",
      "       userId  movieId  rating   timestamp\n",
      "0           1      801     5.0   964982400\n",
      "1           1     2802     4.0   964980694\n",
      "2           1     2765     5.0   964981909\n",
      "3           1      136     5.0   964983650\n",
      "4           1      787     3.0   964982903\n",
      "...       ...      ...     ...         ...\n",
      "80684     611     9190     1.0  1732147438\n",
      "80685     611     8218     4.0  1732147438\n",
      "80686     611     5335     1.5  1732147438\n",
      "80687     611     6522     2.5  1732147438\n",
      "80688     611      472     2.0  1732147438\n",
      "\n",
      "[80689 rows x 4 columns]\n",
      "Новый пользователь с ID 611 добавлен!\n",
      "Оценки нового пользователя:\n",
      "    userId  movieId  rating   timestamp\n",
      "0      611      337     2.5  1732147438\n",
      "1      611     8218     4.0  1732147438\n",
      "2      611     9190     1.0  1732147438\n",
      "3      611      886     2.0  1732147438\n",
      "4      611     1459     1.5  1732147438\n",
      "5      611      484     1.5  1732147438\n",
      "6      611     2577     1.5  1732147438\n",
      "7      611      126     2.5  1732147438\n",
      "8      611     6522     2.5  1732147438\n",
      "9      611     1702     3.0  1732147438\n",
      "10     611     6153     2.5  1732147438\n",
      "11     611     6054     3.5  1732147438\n",
      "12     611      382     2.0  1732147438\n",
      "13     611     7676     4.5  1732147438\n",
      "14     611     2806     1.0  1732147438\n",
      "15     611     6648     3.5  1732147438\n",
      "16     611     7305     2.5  1732147438\n",
      "17     611     3884     3.0  1732147438\n",
      "18     611     5335     1.5  1732147438\n",
      "19     611      472     2.0  1732147438\n"
     ]
    }
   ],
   "source": [
    "# Для загрузки датасета напишем свою реализацию класса Dataset\n",
    "class MovielensDataset(Dataset):\n",
    "    r\"\"\"seed должен быть одинаковым для обучающей и тренировочной выборки\"\"\"\n",
    "    def __init__(self, source, train=True, seed=1):\n",
    "        ratings      = pd.read_csv(rf\"{source}\\ratings.csv\")\n",
    "        self.movies  = pd.read_csv(rf\"{source}\\movies.csv\")\n",
    "\n",
    "        # Преобразовываем Id фильмов в индексы в таблице movies\n",
    "        x = self.movies.loc[:,['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # делим датасет 80% на 20%\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data  = ratings.drop(train_data.index)\n",
    "\n",
    "        self.ratings = train_data if train else test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "        return {\n",
    "            \"user\": torch.LongTensor([sample['userId']]),\n",
    "            \"movie\": torch.LongTensor([sample['movieId']]),\n",
    "            \"rating\": torch.FloatTensor([sample['rating']])\n",
    "        }\n",
    "    def add_new_user(self, seed=1, num_movies=20):\n",
    "        \"\"\"Метод для добавления нового пользователя с его оценками\"\"\"\n",
    "        # Уникальный идентификатор для нового пользователя\n",
    "        new_user_id = self.ratings['userId'].max() + 1\n",
    "    \n",
    "        # Выбор случайных фильмов и присвоение им случайных рейтингов от 1.0 до 5.0\n",
    "        np.random.seed(seed)\n",
    "        sampled_movie_ids = self.ratings['movieId'].unique()[:20]\n",
    "        sampled_ratings = np.round(np.random.uniform(1.0, 5.0, num_movies) * 2) / 2\n",
    "    \n",
    "        #Создание данных для нового пользователя\n",
    "        new_user_ratings = pd.DataFrame({\n",
    "            'userId': new_user_id,\n",
    "            'movieId': sampled_movie_ids,\n",
    "            'rating': sampled_ratings,\n",
    "            'timestamp': int(pd.Timestamp.now().timestamp())\n",
    "        })\n",
    "    \n",
    "        # Добавление нового пользователя в рейтинг\n",
    "        self.ratings = pd.concat([self.ratings, new_user_ratings], ignore_index=False)\n",
    "    \n",
    "        # Возвращаем новый идентификатор пользователя\n",
    "        return new_user_id\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "sataset_source = r'.\\ml-latest-small'\n",
    "\n",
    "movielens_train = MovielensDataset(sataset_source, train=True)\n",
    "movielens_test  = MovielensDataset(sataset_source, train=False)\n",
    "\n",
    "print(\"До добавления нового пользователя:\")\n",
    "print(movielens_train.ratings['userId'].min(), movielens_train.ratings['userId'].max())\n",
    "print(movielens_train.ratings['movieId'].min(), movielens_train.ratings['movieId'].max())\n",
    "print(movielens_train.ratings.sort_values(by='userId').reset_index(drop=True))\n",
    "\n",
    "# Добавление нового пользователя\n",
    "new_user_id = movielens_train.add_new_user()\n",
    "\n",
    "print(\"После добавления нового пользователя:\")\n",
    "print(movielens_train.ratings['userId'].min(), movielens_train.ratings['userId'].max())\n",
    "print(movielens_train.ratings['movieId'].min(), movielens_train.ratings['movieId'].max())\n",
    "print(movielens_train.ratings.sort_values(by='userId').reset_index(drop=True))\n",
    "\n",
    "# Проверим, добавился ли новый пользователь\n",
    "new_user_id = movielens_train.ratings['userId'].max()  # Это должен быть ID нового пользователя\n",
    "\n",
    "# Проверим, есть ли оценки для нового пользователя\n",
    "new_user_ratings = movielens_train.ratings[movielens_train.ratings['userId'] == new_user_id]\n",
    "\n",
    "print(f\"Новый пользователь с ID {new_user_id} добавлен!\")\n",
    "print(\"Оценки нового пользователя:\")\n",
    "print(new_user_ratings)\n",
    "\n",
    "train_loader = DataLoader(movielens_train, batch_size, True)\n",
    "test_loader = DataLoader(movielens_test, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user torch.Size([200, 1])\n",
      "movie torch.Size([200, 1])\n",
      "rating torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для обучения из прошлой лабы, с учётом юзеров и айтемов\n",
    "# Функции для обучения из прошлой лабы, с учётом юзеров и айтемов\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer, sheduler):\n",
    "    model.train()\n",
    "    train_size = len(data_loader.dataset)\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(batch)\n",
    "        loss = loss_function(pred, batch['rating'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            loss, current = loss.item(), (idx + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "def train(model, train_loader, loss_function, optimizer, scheduler, epochs):\n",
    "    for t in range(epochs):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        model.train() # Переключаем сеть в режим обучения\n",
    "\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        train_size = len(train_loader.dataset)\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}   # переносим наши данные на девайс\n",
    "    \n",
    "            pred = model(batch)                 # вычисляем предсказание модели\n",
    "            loss = loss_function(pred, batch['rating'])   # вычисляем потери\n",
    "            loss.backward()                 # запускаем подсчёт градиентов\n",
    "    \n",
    "            optimizer.step()                # делаем шаг градиентного спуска\n",
    "            optimizer.zero_grad()           # обнуляем градиенты\n",
    "    \n",
    "            if idx % 100 == 0:\n",
    "                \n",
    "                lossa, current = loss.item(), (idx + 1) * batch_size\n",
    "                print(f\"loss: {lossa:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Средние потери за эпоху\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)  # Передаем avg_loss в планировщик для корректной работы\n",
    "                \n",
    "        test(model, test_loader, loss_function)\n",
    "\n",
    "#Функция для вывода статистики на тестовом датасете\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    test_size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            pred = model(batch)\n",
    "            loss += loss_function(pred, batch['rating']).item()\n",
    "            correct += (pred.argmax(1) == batch['rating']).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= test_size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричные разложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матричных разложениях используется таблица юзеров-айтемов -- таблица, где по строкам находятся юзеры, по столбцам айтемы, на пересечениях оценка, которую поставил пользователь.\n",
    "\n",
    "Эта таблица представляется в виде произведения двух матриц, матрицы пользователей и матрицы айтемов\n",
    "\n",
    "![разложение](images/PQ.drawio.png)\n",
    "\n",
    "В каждом столбце матрицы пользователей живёт вектор, соответствующий этому пользователю, в матрице айтема, соответственно, вектор айтема. Чтобы получить предсказание оценки, надо их перемножить.\n",
    "\n",
    "Есть много разных способов находить матричные разложения, поскольку у нас тут pytorch, мы просто возьмём два `Embedding` слоя, перемножим, и скажем что это наша модель, которую обучим градиентным спуском\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'scheduler' and 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m mf_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     15\u001b[0m mf_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(mf_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmf_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmf_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'scheduler' and 'epochs'"
     ]
    }
   ],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.user_embeddings(batch['user'])\n",
    "        user_emb = self.movie_embeddings(batch['movie'])\n",
    "        return (movie_emb * user_emb).sum(2)\n",
    "\n",
    "\n",
    "mf_model = MatrixFactorization().to(device)\n",
    "mf_loss = nn.MSELoss()\n",
    "mf_optimizer = torch.optim.SGD(mf_model.parameters(), lr=1)\n",
    "\n",
    "train(10, mf_model, mf_loss, mf_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, если к этой моделе в сумму добавить общую константу и константу для кадого пользователя и айтема, мы получим Factorization Machine\n",
    "\n",
    "https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf\n",
    "\n",
    "А это значит, что помимо эмбедингов с юзерами и айтемами мы можем легко добавить дополнительных параметров! (например тех, что у нас в таблице tags.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFM это расширение обычной Factorization Machine для использования \n",
    "\n",
    "https://arxiv.org/pdf/1703.04247\n",
    "\n",
    "Идея состоит в том, чтобы расположить рядом с обычной Factorization Machine нейронную сеть, которая будет параллельно существовать с матричным разложением\n",
    "\n",
    "![DeepFM](images/DeepFM.png)\n",
    "\n",
    "До DeepFM уже были модели, которые предварительно обучали эмбединги на матричном разложении, а потом использовали их как входные векторы сети, но тут предлагается обучать их сразу совместно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(16*3, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.flatten(self.user_embeddings(batch['user']))\n",
    "        user_emb  = self.flatten(self.movie_embeddings(batch['movie']))\n",
    "\n",
    "        fm = movie_emb * user_emb\n",
    "\n",
    "        deep = torch.cat([movie_emb, user_emb], 1)\n",
    "        deep = self.deep_layers(deep)\n",
    "\n",
    "        v = torch.cat([fm, deep], 1)\n",
    "        v = self.final_layer(v)\n",
    "        # делаем сигмоиду на выходе и масштабируем к оценкам от 0 до 5\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n",
    "\n",
    "deep_mf_model = DeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.SGD(deep_mf_model.parameters(), lr=1e-1)\n",
    "\n",
    "train(5, deep_mf_model, deep_mf_loss, deep_mf_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и более прокаченные версии машины факторизации на нейронках, например xDeepFM\n",
    "\n",
    "https://arxiv.org/pdf/1803.05170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное задание:\n",
    "1) Достичь меньше чем 0.8 значения MSELoss на этом датасете (5 баллов)\n",
    "2) МОЖНО ДЕЛАТЬ ТОЛЬКО ПОСЛЕ ТОГО КАК СДЕЛАНО ПЕРВОЕ ЗАДАНИЕ!  \n",
    "    Добавить в тренировочный датасет нового пользователя - себя и дать оценки минимум 20 фильмов, обучить модель с учётом этого пользователя и сделать для себя рекомендации. (5 баллов) (пожалуйста, не дописывайте себя в файлик, сделайте пользователя добавление в питоне)\n",
    "\n",
    "Дополнительные задания:\n",
    "1) Добавить в модель использование тегов из таблички `tags.csv` (5 дополнительных баллов)\n",
    "2) Добавить в модель использование дополнительных данных из источников `links.csv` (5 дополнительных баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 3.160431  [  200/80689]\n",
      "loss: 1.446992  [20200/80689]\n",
      "loss: 1.018417  [40200/80689]\n",
      "loss: 1.314373  [60200/80689]\n",
      "loss: 0.986642  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.034463 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 0.876477  [  200/80689]\n",
      "loss: 0.954772  [20200/80689]\n",
      "loss: 0.991723  [40200/80689]\n",
      "loss: 1.203809  [60200/80689]\n",
      "loss: 1.054879  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.980897 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 1.003368  [  200/80689]\n",
      "loss: 1.178697  [20200/80689]\n",
      "loss: 0.871857  [40200/80689]\n",
      "loss: 1.098128  [60200/80689]\n",
      "loss: 1.004603  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.952163 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 0.856993  [  200/80689]\n",
      "loss: 0.820415  [20200/80689]\n",
      "loss: 0.883067  [40200/80689]\n",
      "loss: 0.881918  [60200/80689]\n",
      "loss: 0.945334  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.941201 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.902015  [  200/80689]\n",
      "loss: 0.961218  [20200/80689]\n",
      "loss: 1.054953  [40200/80689]\n",
      "loss: 0.887488  [60200/80689]\n",
      "loss: 0.993322  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.914252 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 0.825614  [  200/80689]\n",
      "loss: 0.949089  [20200/80689]\n",
      "loss: 0.936207  [40200/80689]\n",
      "loss: 0.918935  [60200/80689]\n",
      "loss: 0.871177  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.900063 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.992475  [  200/80689]\n",
      "loss: 0.978436  [20200/80689]\n",
      "loss: 0.825327  [40200/80689]\n",
      "loss: 0.741851  [60200/80689]\n",
      "loss: 0.960997  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.883617 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 0.830831  [  200/80689]\n",
      "loss: 0.829736  [20200/80689]\n",
      "loss: 0.806313  [40200/80689]\n",
      "loss: 0.952606  [60200/80689]\n",
      "loss: 0.927253  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.863642 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.961982  [  200/80689]\n",
      "loss: 0.889521  [20200/80689]\n",
      "loss: 0.799949  [40200/80689]\n",
      "loss: 0.875408  [60200/80689]\n",
      "loss: 0.787928  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.838683 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.864890  [  200/80689]\n",
      "loss: 0.883484  [20200/80689]\n",
      "loss: 0.806840  [40200/80689]\n",
      "loss: 0.753466  [60200/80689]\n",
      "loss: 0.981163  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.825811 \n",
      "\n",
      "== Epoch 11 ==\n",
      "loss: 0.758631  [  200/80689]\n",
      "loss: 0.869528  [20200/80689]\n",
      "loss: 0.782801  [40200/80689]\n",
      "loss: 0.635318  [60200/80689]\n",
      "loss: 0.911964  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.810563 \n",
      "\n",
      "== Epoch 12 ==\n",
      "loss: 0.758166  [  200/80689]\n",
      "loss: 0.842460  [20200/80689]\n",
      "loss: 0.728909  [40200/80689]\n",
      "loss: 0.704531  [60200/80689]\n",
      "loss: 0.720411  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.800745 \n",
      "\n",
      "== Epoch 13 ==\n",
      "loss: 0.641870  [  200/80689]\n",
      "loss: 0.789331  [20200/80689]\n",
      "loss: 0.687500  [40200/80689]\n",
      "loss: 0.841549  [60200/80689]\n",
      "loss: 0.755332  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.794618 \n",
      "\n",
      "== Epoch 14 ==\n",
      "loss: 0.775494  [  200/80689]\n",
      "loss: 0.870418  [20200/80689]\n",
      "loss: 0.865057  [40200/80689]\n",
      "loss: 0.599473  [60200/80689]\n",
      "loss: 0.683816  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.785912 \n",
      "\n",
      "== Epoch 15 ==\n",
      "loss: 0.681345  [  200/80689]\n",
      "loss: 0.546079  [20200/80689]\n",
      "loss: 0.642497  [40200/80689]\n",
      "loss: 0.649833  [60200/80689]\n",
      "loss: 0.592479  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.778988 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#улучшенная deepfm\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embeddings = nn.Embedding(1001, 16)\n",
    "        self.movie_embeddings = nn.Embedding(10020, 16)\n",
    "\n",
    "        # Deep layers with increased layer sizes, Dropout, and Batch Normalization\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        self.final_layer = nn.Linear(33, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Получаем эмбеддинги для пользователя и фильма и выравниваем их\n",
    "        user_emb = self.user_embeddings(batch['user']).view(batch['user'].size(0), -1)  # [batch_size, 16]\n",
    "        movie_emb = self.movie_embeddings(batch['movie']).view(batch['movie'].size(0), -1)  # [batch_size, 16]\n",
    "    \n",
    "        # Factorization Machine (FM) компонент — поэлементное умножение и сумма\n",
    "        fm = (user_emb * movie_emb).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "    \n",
    "        # Deep компонент\n",
    "        deep = torch.cat([user_emb, movie_emb], dim=1)  # [batch_size, 32]\n",
    "        deep = self.deep_layers(deep)  # [batch_size, 32]\n",
    "    \n",
    "        # Убедимся, что размеры совпадают для конкатенации\n",
    "        v = torch.cat([fm, deep], dim=1)  # Конкатенируем по dim=1: [batch_size, 33]\n",
    "        v = self.final_layer(v)  # [batch_size, 1]\n",
    "    \n",
    "        # Sigmoid активация для выхода в диапазоне [0, 5]\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss, optimizer, and learning rate scheduler\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "deep_mf_model = DeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.Adam(deep_mf_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(deep_mf_optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "train(deep_mf_model, train_loader, deep_mf_loss, deep_mf_optimizer, scheduler, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 рекомендаций для пользователя 610:\n",
      "Movie ID: 277, Predicted Rating: 4.54\n",
      "Movie ID: 4909, Predicted Rating: 4.52\n",
      "Movie ID: 694, Predicted Rating: 4.51\n",
      "Movie ID: 922, Predicted Rating: 4.49\n",
      "Movie ID: 46, Predicted Rating: 4.47\n",
      "Movie ID: 461, Predicted Rating: 4.47\n",
      "Movie ID: 863, Predicted Rating: 4.46\n",
      "Movie ID: 899, Predicted Rating: 4.45\n",
      "Movie ID: 914, Predicted Rating: 4.43\n",
      "Movie ID: 982, Predicted Rating: 4.43\n"
     ]
    }
   ],
   "source": [
    "def get_movie_recommendations(model, user_id, n_recommendations=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Получить рекомендации для конкретного пользователя.\n",
    "\n",
    "    :param model: Обученная модель\n",
    "    :param user_id: ID пользователя\n",
    "    :param n_recommendations: Количество рекомендаций\n",
    "    :param device: Устройство ('cuda' или 'cpu')\n",
    "    :return: Список фильмов с рекомендованными рейтингами\n",
    "    \"\"\"\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    with torch.no_grad():\n",
    "        # Получаем эмбеддинги для пользователя\n",
    "        user_tensor = torch.LongTensor([user_id]).to(device)\n",
    "        \n",
    "        # Генерация всех возможных movieId\n",
    "        all_movie_ids = torch.LongTensor(np.arange(10000)).to(device)  # 10000 - количество фильмов\n",
    "        \n",
    "        # Строим батч для вычисления предсказаний\n",
    "        batch = {'user': user_tensor.repeat(len(all_movie_ids), 1), 'movie': all_movie_ids.view(-1, 1)}\n",
    "        \n",
    "        # Прогоняем через модель для получения рейтингов\n",
    "        predictions = model(batch).cpu().numpy()\n",
    "        \n",
    "        # Выбираем топ-N фильмов по предсказанным рейтингам\n",
    "        top_n_indices = np.argsort(predictions.flatten())[-n_recommendations:][::-1].copy()\n",
    "        \n",
    "        # Создаем массив идентификаторов фильмов на основе полученных индексов\n",
    "        top_n_movie_ids = all_movie_ids[top_n_indices].numpy()\n",
    "\n",
    "        return top_n_movie_ids, predictions.flatten()[top_n_indices]\n",
    "\n",
    "# Пример использования:\n",
    "user_id = 610  # ID пользователя, для которого генерируем рекомендации\n",
    "n_recommendations = 10  # Количество рекомендаций\n",
    "\n",
    "# Получаем рекомендации\n",
    "recommended_movie_ids, predicted_ratings = get_movie_recommendations(deep_mf_model, user_id, n_recommendations, device)\n",
    "\n",
    "# Выводим рекомендации\n",
    "print(f\"Топ-{n_recommendations} рекомендаций для пользователя {user_id}:\")\n",
    "for movie_id, rating in zip(recommended_movie_ids, predicted_ratings):\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусное изменение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newDeepFM(nn.Module):\n",
    "    def __init__(self, tag_embedding_size=8, max_tags=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.user_embeddings = nn.Embedding(1001, 16)  # 1001 пользователей, 16 размерность\n",
    "        self.movie_embeddings = nn.Embedding(10020, 16)  # 10020 фильмов, 16 размерность\n",
    "        self.tag_embeddings = nn.Embedding(1000, tag_embedding_size, padding_idx=0)  # 1000 тегов, 8 размерность\n",
    "\n",
    "        # Deep layers\n",
    "        input_size = 16 + 16 + tag_embedding_size + 2  # Усредняем теги, поэтому только tag_embedding_size\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        self.final_layer = nn.Linear(33, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Embeddings\n",
    "        user_emb = self.user_embeddings(batch['user']).view(batch['user'].size(0), -1)  # [batch_size, 16]\n",
    "        movie_emb = self.movie_embeddings(batch['movie']).view(batch['movie'].size(0), -1)  # [batch_size, 16]\n",
    "    \n",
    "        # Тег-эмбеддинги: обработка тегов\n",
    "        tags = batch['tags'].clamp(0, self.tag_embeddings.num_embeddings - 1)  # Ограничиваем индексы\n",
    "        tag_emb = self.tag_embeddings(tags)  # [batch_size, max_tags, tag_embedding_size]\n",
    "        tag_emb = tag_emb.mean(dim=1)  # Усреднение по тегам, [batch_size, tag_embedding_size]\n",
    "    \n",
    "        # Дополнительные фичи\n",
    "        link_features = batch['link_features']  # [batch_size, 2]\n",
    "    \n",
    "        # FM Component\n",
    "        fm = (user_emb * movie_emb).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "    \n",
    "        # Deep Component\n",
    "        deep = torch.cat([user_emb, movie_emb, tag_emb, link_features], dim=1)  # [batch_size, input_size]\n",
    "        deep = self.deep_layers(deep)  # [batch_size, 32]\n",
    "    \n",
    "        # Combine FM and Deep components\n",
    "        v = torch.cat([fm, deep], dim=1)  # [batch_size, 33]\n",
    "        v = self.final_layer(v)  # [batch_size, 1]\n",
    "    \n",
    "        # Output scaled to [0, 5]\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, source, train=True, seed=1, max_tags=5):\n",
    "        ratings = pd.read_csv(rf\"{source}\\ratings.csv\")\n",
    "        self.movies = pd.read_csv(rf\"{source}\\movies.csv\")\n",
    "        self.tags = pd.read_csv(rf\"{source}\\tags.csv\")\n",
    "        self.links = pd.read_csv(rf\"{source}\\links.csv\")\n",
    "\n",
    "        # Преобразуем ID фильмов в индексы\n",
    "        x = self.movies.loc[:, ['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "        self.links['movieId'] = self.links['movieId'].map(x.to_dict()['movieId'])\n",
    "        self.tags['movieId'] = self.tags['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # Словарь тегов для каждого фильма\n",
    "        self.movie_to_tags = self._preprocess_tags(max_tags)\n",
    "\n",
    "        # Нормализуем данные links\n",
    "        self.link_features = self._preprocess_links()\n",
    "\n",
    "        # Разделение на обучающую и тестовую выборки\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data = ratings.drop(train_data.index)\n",
    "        self.ratings = train_data if train else test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "\n",
    "        # Теги\n",
    "        tags = self.movie_to_tags.get(sample['movieId'], [0] * 5)  # Паддинг нулями\n",
    "\n",
    "        # Фичи из links.csv\n",
    "        link_features = self.link_features[sample['movieId']]\n",
    "\n",
    "        return {\n",
    "            \"user\": torch.LongTensor([sample['userId']]),\n",
    "            \"movie\": torch.LongTensor([sample['movieId']]),\n",
    "            \"rating\": torch.FloatTensor([sample['rating']]),\n",
    "            \"tags\": torch.LongTensor(tags),  # Теги как индексы\n",
    "            \"link_features\": torch.FloatTensor(link_features)  # Нормализованные фичи\n",
    "        }\n",
    "\n",
    "    def _preprocess_tags(self, max_tags):\n",
    "        \"\"\"Преобразование тегов в индексы.\"\"\"\n",
    "        unique_tags = list(set(self.tags['tag'].values))\n",
    "        tag_to_idx = {tag: idx + 1 for idx, tag in enumerate(unique_tags)}  # 0 для паддинга\n",
    "\n",
    "        # Формируем словарь {movieId: [tag_idx, ...]}\n",
    "        movie_to_tags = defaultdict(list)\n",
    "        for _, row in self.tags.iterrows():\n",
    "            movie_to_tags[row['movieId']].append(tag_to_idx[row['tag']])\n",
    "\n",
    "        # Ограничиваем количество тегов\n",
    "        for movie_id, tag_list in movie_to_tags.items():\n",
    "            movie_to_tags[movie_id] = tag_list[:max_tags] + [0] * (max_tags - len(tag_list))\n",
    "\n",
    "        return movie_to_tags\n",
    "\n",
    "    def _preprocess_links(self):\n",
    "        \"\"\"Нормализация данных из links.csv.\"\"\"\n",
    "        features = self.links[['imdbId', 'tmdbId']].fillna(0).values\n",
    "        max_vals = features.max(axis=0)\n",
    "        features = features / max_vals  # Нормализация в диапазоне [0, 1]\n",
    "\n",
    "        # Создаем словарь {movieId: [normalized_features]}\n",
    "        link_features = {row['movieId']: features[i] for i, row in self.links.iterrows()}\n",
    "        return link_features\n",
    "\n",
    "    def add_new_user(self, seed=1, num_movies=20):\n",
    "        \"\"\"Добавление нового пользователя с его оценками.\"\"\"\n",
    "        new_user_id = self.ratings['userId'].max() + 1\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        sampled_movie_ids = self.ratings['movieId'].unique()[:20]\n",
    "        sampled_ratings = np.round(np.random.uniform(1.0, 5.0, num_movies) * 2) / 2\n",
    "\n",
    "        new_user_ratings = pd.DataFrame({\n",
    "            'userId': new_user_id,\n",
    "            'movieId': sampled_movie_ids,\n",
    "            'rating': sampled_ratings,\n",
    "            'timestamp': int(pd.Timestamp.now().timestamp())\n",
    "        })\n",
    "\n",
    "        self.ratings = pd.concat([self.ratings, new_user_ratings], ignore_index=False)\n",
    "        return new_user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "sataset_source = r'.\\ml-latest-small'\n",
    "\n",
    "movielens_train = MovielensDataset(sataset_source, train=True)\n",
    "movielens_test  = MovielensDataset(sataset_source, train=False)\n",
    "\n",
    "train_loader = DataLoader(movielens_train, batch_size, True)\n",
    "test_loader = DataLoader(movielens_test, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags in batch: tensor([[999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [263,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [443, 999,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [440, 875, 999, 999,   0],\n",
      "        [998, 861,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [805, 361, 999, 389, 243],\n",
      "        [758, 758, 217,   0,   0],\n",
      "        [611, 999, 999,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [235, 530, 598,  34,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 190, 999, 918, 886],\n",
      "        [999,  91, 999, 999, 999],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 31, 133, 999, 999, 827],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [944,   0,   0,   0,   0],\n",
      "        [516, 231, 999, 952,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 999, 999, 782, 999],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [493,   0,   0,   0,   0],\n",
      "        [432, 999,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [723, 999, 999, 999, 858],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 902, 918, 756, 670],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 31, 133, 999, 999, 827],\n",
      "        [371, 999, 466,   0,   0],\n",
      "        [710, 469,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [496,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [123,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 61,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [693,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [241,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,  91, 999, 999, 999],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [549,   0,   0,   0,   0],\n",
      "        [773,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 710, 697, 999, 999],\n",
      "        [507,   0,   0,   0,   0],\n",
      "        [999, 776, 935, 999, 999],\n",
      "        [782, 999, 862, 999, 101],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [202, 999, 124, 752, 999],\n",
      "        [537,  21, 999, 999,  33],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [891, 999,   0,   0,   0],\n",
      "        [999, 999,  91, 474, 999],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [710,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 847, 544, 999, 858],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [805,  46, 625, 999, 434],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [998, 861,   0,   0,   0],\n",
      "        [815,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [274,   0,   0,   0,   0],\n",
      "        [123,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 85, 999,  85,   0,   0],\n",
      "        [758, 758, 217,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [367,  26,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 999, 862, 999,  13],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [889, 999, 970, 381, 999],\n",
      "        [543,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [460,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [123, 123,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [333,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [127, 591, 944, 752, 999],\n",
      "        [999,   0,   0,   0,   0],\n",
      "        [999, 999, 999,   0,   0],\n",
      "        [875,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 26, 831, 999, 756, 285],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [371, 999, 466,   0,   0],\n",
      "        [847, 937, 537, 999,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 12, 590,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [891, 999,   0,   0,   0],\n",
      "        [376, 670, 517, 999, 336],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [112,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [999, 830, 999, 884, 999],\n",
      "        [493, 493, 847, 591, 999],\n",
      "        [999, 466, 112, 365, 999],\n",
      "        [999, 999,  66, 999,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [594,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [756, 862, 947, 999, 182],\n",
      "        [193,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0]])\n",
      "Max tag index: 999\n",
      "Min tag index: 0\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    batch['tags'] = batch['tags'].clamp(0, 999)\n",
    "    print(\"Tags in batch:\", batch['tags'])\n",
    "    print(\"Max tag index:\", batch['tags'].max().item())\n",
    "    print(\"Min tag index:\", batch['tags'].min().item())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 4.171557  [  200/80669]\n",
      "loss: 1.118802  [20200/80669]\n",
      "loss: 0.999063  [40200/80669]\n",
      "loss: 1.134953  [60200/80669]\n",
      "loss: 1.070572  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.021810 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 0.973078  [  200/80669]\n",
      "loss: 1.152095  [20200/80669]\n",
      "loss: 0.954809  [40200/80669]\n",
      "loss: 1.006898  [60200/80669]\n",
      "loss: 0.970310  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.969078 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 0.918209  [  200/80669]\n",
      "loss: 1.140400  [20200/80669]\n",
      "loss: 0.959102  [40200/80669]\n",
      "loss: 1.033707  [60200/80669]\n",
      "loss: 0.819741  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.943036 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 0.915161  [  200/80669]\n",
      "loss: 0.916775  [20200/80669]\n",
      "loss: 0.803124  [40200/80669]\n",
      "loss: 1.080952  [60200/80669]\n",
      "loss: 0.784749  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.908509 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 1.240274  [  200/80669]\n",
      "loss: 0.892292  [20200/80669]\n",
      "loss: 0.804601  [40200/80669]\n",
      "loss: 0.883409  [60200/80669]\n",
      "loss: 1.097182  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.893994 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 0.865764  [  200/80669]\n",
      "loss: 0.894360  [20200/80669]\n",
      "loss: 0.863139  [40200/80669]\n",
      "loss: 0.774778  [60200/80669]\n",
      "loss: 1.033614  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.870065 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.809028  [  200/80669]\n",
      "loss: 0.950021  [20200/80669]\n",
      "loss: 0.919003  [40200/80669]\n",
      "loss: 0.842325  [60200/80669]\n",
      "loss: 1.047715  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.861776 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 0.754343  [  200/80669]\n",
      "loss: 0.827073  [20200/80669]\n",
      "loss: 0.857729  [40200/80669]\n",
      "loss: 0.732531  [60200/80669]\n",
      "loss: 0.772851  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.844664 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.760576  [  200/80669]\n",
      "loss: 0.777086  [20200/80669]\n",
      "loss: 0.770850  [40200/80669]\n",
      "loss: 0.967388  [60200/80669]\n",
      "loss: 0.704425  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.831710 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.616194  [  200/80669]\n",
      "loss: 0.926781  [20200/80669]\n",
      "loss: 0.700321  [40200/80669]\n",
      "loss: 0.799583  [60200/80669]\n",
      "loss: 1.014800  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.818625 \n",
      "\n",
      "== Epoch 11 ==\n",
      "loss: 1.141610  [  200/80669]\n",
      "loss: 0.908858  [20200/80669]\n",
      "loss: 0.702605  [40200/80669]\n",
      "loss: 0.622577  [60200/80669]\n",
      "loss: 0.819998  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.805621 \n",
      "\n",
      "== Epoch 12 ==\n",
      "loss: 0.707287  [  200/80669]\n",
      "loss: 0.761811  [20200/80669]\n",
      "loss: 0.755603  [40200/80669]\n",
      "loss: 0.685775  [60200/80669]\n",
      "loss: 0.662040  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.789126 \n",
      "\n",
      "== Epoch 13 ==\n",
      "loss: 0.741459  [  200/80669]\n",
      "loss: 0.712163  [20200/80669]\n",
      "loss: 0.681222  [40200/80669]\n",
      "loss: 0.776141  [60200/80669]\n",
      "loss: 0.778416  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.783164 \n",
      "\n",
      "== Epoch 14 ==\n",
      "loss: 0.488126  [  200/80669]\n",
      "loss: 0.671336  [20200/80669]\n",
      "loss: 0.703839  [40200/80669]\n",
      "loss: 0.710576  [60200/80669]\n",
      "loss: 0.532181  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.778421 \n",
      "\n",
      "== Epoch 15 ==\n",
      "loss: 0.614205  [  200/80669]\n",
      "loss: 0.606232  [20200/80669]\n",
      "loss: 0.761865  [40200/80669]\n",
      "loss: 0.815478  [60200/80669]\n",
      "loss: 0.576289  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.770786 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "deep_mf_model = newDeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.Adam(deep_mf_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(deep_mf_optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "train(deep_mf_model, train_loader, deep_mf_loss, deep_mf_optimizer, scheduler, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
