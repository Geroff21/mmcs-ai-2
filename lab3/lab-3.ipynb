{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-3. Рекомендательные системы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tconst  averageRating  numVotes\n",
      "1506745  tt9916730            7.0        12\n",
      "1506746  tt9916766            7.1        24\n",
      "1506747  tt9916778            7.2        37\n",
      "1506748  tt9916840            6.9        11\n",
      "1506749  tt9916880            7.9         9\n",
      "          tconst  averageRating  numVotes\n",
      "87209  tt0114709            8.3   1097902\n"
     ]
    }
   ],
   "source": [
    "# Чтение файла .tsv\n",
    "data = pd.read_csv('ml-latest-small/title.ratings.tsv', sep='\\t')\n",
    "\n",
    "# Вывод первых строк\n",
    "print(data.tail())\n",
    "\n",
    "# Условие для фильтрации\n",
    "tconst_value = 'tt0114709'\n",
    "result = data[data['tconst'] == tconst_value]\n",
    "\n",
    "# Вывод результата\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Выбираем девайс\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать MovieLens\n",
    "\n",
    "https://grouplens.org/datasets/movielens/\n",
    "\n",
    "А именно, самый маленький вариант со 100 тыс. оценок\n",
    "\n",
    "https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До добавления нового пользователя:\n",
      "1 610\n",
      "0 9741\n",
      "       userId  movieId  rating   timestamp\n",
      "0           1     1319     4.0   964981230\n",
      "1           1      801     5.0   964982400\n",
      "2           1     2608     4.0   964981775\n",
      "3           1      981     5.0   964982703\n",
      "4           1      275     3.0   964982310\n",
      "...       ...      ...     ...         ...\n",
      "80664     610     8686     4.0  1479542606\n",
      "80665     610     9389     3.5  1493848789\n",
      "80666     610     2970     2.0  1493849562\n",
      "80667     610     2916     4.5  1493847010\n",
      "80668     610     5640     5.0  1479545132\n",
      "\n",
      "[80669 rows x 4 columns]\n",
      "После добавления нового пользователя:\n",
      "1 611\n",
      "0 9741\n",
      "       userId  movieId  rating   timestamp\n",
      "0           1      801     5.0   964982400\n",
      "1           1     2802     4.0   964980694\n",
      "2           1     2765     5.0   964981909\n",
      "3           1      136     5.0   964983650\n",
      "4           1      787     3.0   964982903\n",
      "...       ...      ...     ...         ...\n",
      "80684     611        4     4.0  1733234572\n",
      "80685     611        3     3.0  1733234572\n",
      "80686     611      198     4.5  1733234572\n",
      "80687     611       19     4.5  1733234572\n",
      "80688     611      208     4.0  1733234572\n",
      "\n",
      "[80689 rows x 4 columns]\n",
      "Новый пользователь с ID 611 добавлен!\n",
      "Оценки нового пользователя:\n",
      "       userId  movieId  rating   timestamp\n",
      "80669     611        1     4.5  1733234572\n",
      "80670     611        3     3.0  1733234572\n",
      "80671     611        4     4.0  1733234572\n",
      "80672     611        5     3.5  1733234572\n",
      "80673     611        7     5.0  1733234572\n",
      "80674     611       11     4.0  1733234572\n",
      "80675     611       12     3.5  1733234572\n",
      "80676     611       18     4.0  1733234572\n",
      "80677     611       19     4.5  1733234572\n",
      "80678     611       20     3.0  1733234572\n",
      "80679     611       29     5.0  1733234572\n",
      "80680     611       32     4.5  1733234572\n",
      "80681     611       66     4.0  1733234572\n",
      "80682     611       76     3.5  1733234572\n",
      "80683     611      103     4.0  1733234572\n",
      "80684     611      160     5.0  1733234572\n",
      "80685     611      172     3.5  1733234572\n",
      "80686     611      173     4.0  1733234572\n",
      "80687     611      198     4.5  1733234572\n",
      "80688     611      208     4.0  1733234572\n"
     ]
    }
   ],
   "source": [
    "# Для загрузки датасета напишем свою реализацию класса Dataset\n",
    "class MovielensDataset(Dataset):\n",
    "    r\"\"\"seed должен быть одинаковым для обучающей и тренировочной выборки\"\"\"\n",
    "    def __init__(self, source, train=True, seed=1):\n",
    "        ratings      = pd.read_csv(rf\"{source}\\ratings.csv\")\n",
    "        self.movies  = pd.read_csv(rf\"{source}\\movies.csv\")\n",
    "\n",
    "        # Преобразовываем Id фильмов в индексы в таблице movies\n",
    "        x = self.movies.loc[:,['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # делим датасет 80% на 20%\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data  = ratings.drop(train_data.index)\n",
    "\n",
    "        self.ratings = train_data if train else test_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "        return {\n",
    "            \"user\": torch.LongTensor([sample['userId']]),\n",
    "            \"movie\": torch.LongTensor([sample['movieId']]),\n",
    "            \"rating\": torch.FloatTensor([sample['rating']])\n",
    "        }\n",
    "    def add_new_user(self, movie_ids, ratings, seed=1):\n",
    "\n",
    "        # Проверяем, что количество фильмов совпадает с количеством оценок\n",
    "        if len(movie_ids) != len(ratings):\n",
    "            raise ValueError(\"Количество фильмов и оценок должно совпадать.\")\n",
    "        \n",
    "        # Уникальный идентификатор для нового пользователя\n",
    "        new_user_id = self.ratings['userId'].max() + 1\n",
    "        \n",
    "        # Создаем временную метку\n",
    "        timestamp = int(pd.Timestamp.now().timestamp())\n",
    "        \n",
    "        # Создаем данные для нового пользователя\n",
    "        new_user_ratings = pd.DataFrame({\n",
    "            'userId': [new_user_id] * len(movie_ids),\n",
    "            'movieId': movie_ids,\n",
    "            'rating': ratings,\n",
    "            'timestamp': [timestamp] * len(movie_ids)\n",
    "        })\n",
    "        \n",
    "        # Добавляем данные нового пользователя в рейтинг\n",
    "        self.ratings = pd.concat([self.ratings, new_user_ratings], ignore_index=True)\n",
    "        \n",
    "        # Возвращаем новый идентификатор пользователя\n",
    "        return new_user_id\n",
    "\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "sataset_source = r'.\\ml-latest-small'\n",
    "\n",
    "movielens_train = MovielensDataset(sataset_source, train=True)\n",
    "movielens_test  = MovielensDataset(sataset_source, train=False)\n",
    "\n",
    "print(\"До добавления нового пользователя:\")\n",
    "print(movielens_train.ratings['userId'].min(), movielens_train.ratings['userId'].max())\n",
    "print(movielens_train.ratings['movieId'].min(), movielens_train.ratings['movieId'].max())\n",
    "print(movielens_train.ratings.sort_values(by='userId').reset_index(drop=True))\n",
    "\n",
    "ids = [1, 3, 4, 5, 7, 11, 12, 18, 19, 20, 29, 32, 66, 76, 103, 160, 172, 173, 198, 208]\n",
    "rts = [4.5, 3.0, 4.0, 3.5, 5.0, 4.0, 3.5, 4.0, 4.5, 3.0, 5.0, 4.5, 4.0, 3.5, 4.0, 5.0, 3.5, 4.0, 4.5, 4.0]\n",
    "\n",
    "# Добавление нового пользователя\n",
    "new_user_id = movielens_train.add_new_user(\n",
    "    movie_ids=ids,\n",
    "    ratings=rts\n",
    ")\n",
    "print(\"После добавления нового пользователя:\")\n",
    "print(movielens_train.ratings['userId'].min(), movielens_train.ratings['userId'].max())\n",
    "print(movielens_train.ratings['movieId'].min(), movielens_train.ratings['movieId'].max())\n",
    "print(movielens_train.ratings.sort_values(by='userId').reset_index(drop=True))\n",
    "\n",
    "# Проверим, добавился ли новый пользователь\n",
    "new_user_id = movielens_train.ratings['userId'].max()  # Это должен быть ID нового пользователя\n",
    "\n",
    "# Проверим, есть ли оценки для нового пользователя\n",
    "new_user_ratings = movielens_train.ratings[movielens_train.ratings['userId'] == new_user_id]\n",
    "\n",
    "print(f\"Новый пользователь с ID {new_user_id} добавлен!\")\n",
    "print(\"Оценки нового пользователя:\")\n",
    "print(new_user_ratings)\n",
    "\n",
    "train_loader = DataLoader(movielens_train, batch_size, True)\n",
    "test_loader = DataLoader(movielens_test, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user torch.Size([200, 1])\n",
      "movie torch.Size([200, 1])\n",
      "rating torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(k, v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для обучения из прошлой лабы, с учётом юзеров и айтемов\n",
    "# Функции для обучения из прошлой лабы, с учётом юзеров и айтемов\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer, sheduler):\n",
    "    model.train()\n",
    "    train_size = len(data_loader.dataset)\n",
    "    for idx, batch in enumerate(data_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(batch)\n",
    "        loss = loss_function(pred, batch['rating'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            loss, current = loss.item(), (idx + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "def train(model, train_loader, loss_function, optimizer, scheduler, epochs):\n",
    "    for t in range(epochs):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        model.train() # Переключаем сеть в режим обучения\n",
    "\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        train_size = len(train_loader.dataset)\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}   # переносим наши данные на девайс\n",
    "    \n",
    "            pred = model(batch)                 # вычисляем предсказание модели\n",
    "            loss = loss_function(pred, batch['rating'])   # вычисляем потери\n",
    "            loss.backward()                 # запускаем подсчёт градиентов\n",
    "    \n",
    "            optimizer.step()                # делаем шаг градиентного спуска\n",
    "            optimizer.zero_grad()           # обнуляем градиенты\n",
    "    \n",
    "            if idx % 100 == 0:\n",
    "                \n",
    "                lossa, current = loss.item(), (idx + 1) * batch_size\n",
    "                print(f\"loss: {lossa:>7f}  [{current:>5d}/{train_size:>5d}]\")\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Средние потери за эпоху\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)  # Передаем avg_loss в планировщик для корректной работы\n",
    "                \n",
    "        test(model, test_loader, loss_function)\n",
    "\n",
    "#Функция для вывода статистики на тестовом датасете\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "    test_size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            pred = model(batch)\n",
    "            loss += loss_function(pred, batch['rating']).item()\n",
    "            correct += (pred.argmax(1) == batch['rating']).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= test_size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричные разложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В матричных разложениях используется таблица юзеров-айтемов -- таблица, где по строкам находятся юзеры, по столбцам айтемы, на пересечениях оценка, которую поставил пользователь.\n",
    "\n",
    "Эта таблица представляется в виде произведения двух матриц, матрицы пользователей и матрицы айтемов\n",
    "\n",
    "![разложение](images/PQ.drawio.png)\n",
    "\n",
    "В каждом столбце матрицы пользователей живёт вектор, соответствующий этому пользователю, в матрице айтема, соответственно, вектор айтема. Чтобы получить предсказание оценки, надо их перемножить.\n",
    "\n",
    "Есть много разных способов находить матричные разложения, поскольку у нас тут pytorch, мы просто возьмём два `Embedding` слоя, перемножим, и скажем что это наша модель, которую обучим градиентным спуском\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'scheduler' and 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m mf_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     15\u001b[0m mf_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(mf_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmf_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmf_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'scheduler' and 'epochs'"
     ]
    }
   ],
   "source": [
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.user_embeddings(batch['user'])\n",
    "        user_emb = self.movie_embeddings(batch['movie'])\n",
    "        return (movie_emb * user_emb).sum(2)\n",
    "\n",
    "\n",
    "mf_model = MatrixFactorization().to(device)\n",
    "mf_loss = nn.MSELoss()\n",
    "mf_optimizer = torch.optim.SGD(mf_model.parameters(), lr=1)\n",
    "\n",
    "train(10, mf_model, mf_loss, mf_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фактически, если к этой моделе в сумму добавить общую константу и константу для кадого пользователя и айтема, мы получим Factorization Machine\n",
    "\n",
    "https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf\n",
    "\n",
    "А это значит, что помимо эмбедингов с юзерами и айтемами мы можем легко добавить дополнительных параметров! (например тех, что у нас в таблице tags.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepFM это расширение обычной Factorization Machine для использования \n",
    "\n",
    "https://arxiv.org/pdf/1703.04247\n",
    "\n",
    "Идея состоит в том, чтобы расположить рядом с обычной Factorization Machine нейронную сеть, которая будет параллельно существовать с матричным разложением\n",
    "\n",
    "![DeepFM](images/DeepFM.png)\n",
    "\n",
    "До DeepFM уже были модели, которые предварительно обучали эмбединги на матричном разложении, а потом использовали их как входные векторы сети, но тут предлагается обучать их сразу совместно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 2 required positional arguments: 'scheduler' and 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m deep_mf_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     38\u001b[0m deep_mf_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(deep_mf_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_mf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_mf_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep_mf_optimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 2 required positional arguments: 'scheduler' and 'epochs'"
     ]
    }
   ],
   "source": [
    "class DeepFM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_embeddings  = nn.Embedding(1000,  16)\n",
    "        self.movie_embeddings = nn.Embedding(10000, 16)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(16*3, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        movie_emb = self.flatten(self.user_embeddings(batch['user']))\n",
    "        user_emb  = self.flatten(self.movie_embeddings(batch['movie']))\n",
    "\n",
    "        fm = movie_emb * user_emb\n",
    "\n",
    "        deep = torch.cat([movie_emb, user_emb], 1)\n",
    "        deep = self.deep_layers(deep)\n",
    "\n",
    "        v = torch.cat([fm, deep], 1)\n",
    "        v = self.final_layer(v)\n",
    "        # делаем сигмоиду на выходе и масштабируем к оценкам от 0 до 5\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n",
    "\n",
    "deep_mf_model = DeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.SGD(deep_mf_model.parameters(), lr=1e-1)\n",
    "\n",
    "train(5, deep_mf_model, deep_mf_loss, deep_mf_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть и более прокаченные версии машины факторизации на нейронках, например xDeepFM\n",
    "\n",
    "https://arxiv.org/pdf/1803.05170"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное задание:\n",
    "1) Достичь меньше чем 0.8 значения MSELoss на этом датасете (5 баллов)\n",
    "2) МОЖНО ДЕЛАТЬ ТОЛЬКО ПОСЛЕ ТОГО КАК СДЕЛАНО ПЕРВОЕ ЗАДАНИЕ!  \n",
    "    Добавить в тренировочный датасет нового пользователя - себя и дать оценки минимум 20 фильмов, обучить модель с учётом этого пользователя и сделать для себя рекомендации. (5 баллов) (пожалуйста, не дописывайте себя в файлик, сделайте пользователя добавление в питоне)\n",
    "\n",
    "Дополнительные задания:\n",
    "1) Добавить в модель использование тегов из таблички `tags.csv` (5 дополнительных баллов)\n",
    "2) Добавить в модель использование дополнительных данных из источников `links.csv` (5 дополнительных баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 1.958522  [  200/80689]\n",
      "loss: 0.958518  [20200/80689]\n",
      "loss: 0.979528  [40200/80689]\n",
      "loss: 0.876545  [60200/80689]\n",
      "loss: 1.130323  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.042732 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 1.049050  [  200/80689]\n",
      "loss: 1.002605  [20200/80689]\n",
      "loss: 0.895758  [40200/80689]\n",
      "loss: 0.988102  [60200/80689]\n",
      "loss: 0.767082  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.988196 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 1.066121  [  200/80689]\n",
      "loss: 0.957649  [20200/80689]\n",
      "loss: 1.049770  [40200/80689]\n",
      "loss: 0.862593  [60200/80689]\n",
      "loss: 0.930352  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.960779 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 1.072987  [  200/80689]\n",
      "loss: 0.885884  [20200/80689]\n",
      "loss: 0.898144  [40200/80689]\n",
      "loss: 0.818786  [60200/80689]\n",
      "loss: 0.821711  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.932513 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.991483  [  200/80689]\n",
      "loss: 0.944270  [20200/80689]\n",
      "loss: 0.869380  [40200/80689]\n",
      "loss: 0.938980  [60200/80689]\n",
      "loss: 1.027322  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.907453 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 1.109036  [  200/80689]\n",
      "loss: 0.956517  [20200/80689]\n",
      "loss: 0.744092  [40200/80689]\n",
      "loss: 0.872748  [60200/80689]\n",
      "loss: 0.763612  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.891772 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.990633  [  200/80689]\n",
      "loss: 0.793654  [20200/80689]\n",
      "loss: 1.051171  [40200/80689]\n",
      "loss: 0.905872  [60200/80689]\n",
      "loss: 0.824334  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.868614 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 0.752174  [  200/80689]\n",
      "loss: 0.873356  [20200/80689]\n",
      "loss: 0.807290  [40200/80689]\n",
      "loss: 1.105087  [60200/80689]\n",
      "loss: 0.953156  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.848617 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.794759  [  200/80689]\n",
      "loss: 0.871941  [20200/80689]\n",
      "loss: 0.743416  [40200/80689]\n",
      "loss: 0.853672  [60200/80689]\n",
      "loss: 0.806734  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.831387 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.868083  [  200/80689]\n",
      "loss: 0.759744  [20200/80689]\n",
      "loss: 1.179627  [40200/80689]\n",
      "loss: 0.863020  [60200/80689]\n",
      "loss: 0.754254  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.814937 \n",
      "\n",
      "== Epoch 11 ==\n",
      "loss: 0.819703  [  200/80689]\n",
      "loss: 0.695546  [20200/80689]\n",
      "loss: 0.853399  [40200/80689]\n",
      "loss: 0.692342  [60200/80689]\n",
      "loss: 0.667942  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.808629 \n",
      "\n",
      "== Epoch 12 ==\n",
      "loss: 0.832904  [  200/80689]\n",
      "loss: 0.842223  [20200/80689]\n",
      "loss: 0.660255  [40200/80689]\n",
      "loss: 0.719352  [60200/80689]\n",
      "loss: 0.765589  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.798982 \n",
      "\n",
      "== Epoch 13 ==\n",
      "loss: 0.756642  [  200/80689]\n",
      "loss: 0.666098  [20200/80689]\n",
      "loss: 0.671604  [40200/80689]\n",
      "loss: 0.811545  [60200/80689]\n",
      "loss: 0.714696  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.786925 \n",
      "\n",
      "== Epoch 14 ==\n",
      "loss: 0.729701  [  200/80689]\n",
      "loss: 0.848199  [20200/80689]\n",
      "loss: 0.736716  [40200/80689]\n",
      "loss: 0.790394  [60200/80689]\n",
      "loss: 0.725609  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.787265 \n",
      "\n",
      "== Epoch 15 ==\n",
      "loss: 0.698505  [  200/80689]\n",
      "loss: 0.534350  [20200/80689]\n",
      "loss: 0.646717  [40200/80689]\n",
      "loss: 0.741536  [60200/80689]\n",
      "loss: 0.620182  [80200/80689]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.778766 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#улучшенная deepfm\n",
    "\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.user_embeddings = nn.Embedding(1001, 16)\n",
    "        self.movie_embeddings = nn.Embedding(10020, 16)\n",
    "\n",
    "        # Deep layers with increased layer sizes, Dropout, and Batch Normalization\n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        self.final_layer = nn.Linear(33, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Получаем эмбеддинги для пользователя и фильма и выравниваем их\n",
    "        user_emb = self.user_embeddings(batch['user']).view(batch['user'].size(0), -1)  # [batch_size, 16]\n",
    "        movie_emb = self.movie_embeddings(batch['movie']).view(batch['movie'].size(0), -1)  # [batch_size, 16]\n",
    "    \n",
    "        # Factorization Machine (FM) компонент — поэлементное умножение и сумма\n",
    "        fm = (user_emb * movie_emb).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "    \n",
    "        # Deep компонент\n",
    "        deep = torch.cat([user_emb, movie_emb], dim=1)  # [batch_size, 32]\n",
    "        deep = self.deep_layers(deep)  # [batch_size, 32]\n",
    "    \n",
    "        # Убедимся, что размеры совпадают для конкатенации\n",
    "        v = torch.cat([fm, deep], dim=1)  # Конкатенируем по dim=1: [batch_size, 33]\n",
    "        v = self.final_layer(v)  # [batch_size, 1]\n",
    "    \n",
    "        # Sigmoid активация для выхода в диапазоне [0, 5]\n",
    "        return torch.sigmoid(v) * 5\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model, loss, optimizer, and learning rate scheduler\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "deep_mf_model = DeepFM().to(device)\n",
    "deep_mf_loss = nn.MSELoss()\n",
    "deep_mf_optimizer = torch.optim.Adam(deep_mf_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(deep_mf_optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "train(deep_mf_model, train_loader, deep_mf_loss, deep_mf_optimizer, scheduler, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 рекомендаций для пользователя 611:\n",
      "Movie ID: 694, Title: Substitute, The (1996), Predicted Rating: 4.51\n",
      "Movie ID: 9618 не найден в списке фильмов.\n",
      "Movie ID: 659 не найден в списке фильмов.\n",
      "Movie ID: 1730, Title: Kundun (1997), Predicted Rating: 4.47\n",
      "Movie ID: 277, Title: Miracle on 34th Street (1994), Predicted Rating: 4.47\n",
      "Movie ID: 922, Title: Sunset Blvd. (a.k.a. Sunset Boulevard) (1950), Predicted Rating: 4.46\n",
      "Movie ID: 257, Title: Just Cause (1995), Predicted Rating: 4.45\n",
      "Movie ID: 3622, Title: Twelve Chairs, The (1970), Predicted Rating: 4.45\n",
      "Movie ID: 6153, Title: Zapped! (1982), Predicted Rating: 4.45\n",
      "Movie ID: 944, Title: Lost Horizon (1937), Predicted Rating: 4.44\n"
     ]
    }
   ],
   "source": [
    "def get_movie_recommendations(model, user_id, dataset, n_recommendations=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Получить рекомендации для конкретного пользователя.\n",
    "\n",
    "    :param model: Обученная модель\n",
    "    :param user_id: ID пользователя\n",
    "    :param dataset: Экземпляр датасета (например, MovielensDataset)\n",
    "    :param n_recommendations: Количество рекомендаций\n",
    "    :param device: Устройство ('cuda' или 'cpu')\n",
    "    :return: Список фильмов с рекомендованными рейтингами\n",
    "    \"\"\"\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    with torch.no_grad():\n",
    "        # Получаем список всех фильмов\n",
    "        all_movie_ids = dataset.movies['movieId'].values\n",
    "        \n",
    "        # Проверяем, какие фильмы пользователь уже оценил\n",
    "        rated_movies = dataset.ratings[dataset.ratings['userId'] == user_id]['movieId'].values\n",
    "        \n",
    "        # Фильтруем фильмы, которые пользователь еще не оценил\n",
    "        unseen_movies = [movie_id for movie_id in all_movie_ids if movie_id not in rated_movies]\n",
    "        \n",
    "        # Преобразуем индексы фильмов в индексы для эмбеддингов (если необходимо)\n",
    "        movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(all_movie_ids)}\n",
    "        unseen_movie_indices = [movie_to_idx[movie_id] for movie_id in unseen_movies]\n",
    "\n",
    "        # Получаем эмбеддинги для пользователя\n",
    "        user_tensor = torch.LongTensor([user_id] * len(unseen_movie_indices)).to(device)\n",
    "        movie_tensor = torch.LongTensor(unseen_movie_indices).to(device)\n",
    "        \n",
    "        # Строим батч для вычисления предсказаний\n",
    "        batch = {'user': user_tensor, 'movie': movie_tensor}\n",
    "        \n",
    "        # Прогоняем через модель для получения рейтингов\n",
    "        predictions = model(batch).cpu().numpy()\n",
    "        \n",
    "        # Выбираем топ-N фильмов по предсказанным рейтингам\n",
    "        top_n_indices = np.argsort(predictions.flatten())[-n_recommendations:][::-1]\n",
    "        \n",
    "        # Создаем массив идентификаторов фильмов на основе полученных индексов\n",
    "        top_n_movie_ids = [unseen_movies[i] for i in top_n_indices]\n",
    "        top_n_ratings = predictions.flatten()[top_n_indices]\n",
    "\n",
    "        # Выводим рекомендации с названиями фильмов и предсказанными рейтингами\n",
    "        recommended_movies = []\n",
    "        for movie_id, rating in zip(top_n_movie_ids, top_n_ratings):\n",
    "            movie_title = dataset.movies.loc[dataset.movies['movieId'] == movie_id, 'title']\n",
    "            if movie_title.empty:\n",
    "                recommended_movies.append(f\"Movie ID: {movie_id} не найден в списке фильмов.\")\n",
    "            else:\n",
    "                movie_title = movie_title.values[0]\n",
    "                recommended_movies.append(f\"Movie ID: {movie_id}, Title: {movie_title}, Predicted Rating: {rating:.2f}\")\n",
    "        \n",
    "        return recommended_movies\n",
    "\n",
    "# Пример использования:\n",
    "user_id = 611  # ID пользователя, для которого генерируем рекомендации\n",
    "n_recommendations = 10  # Количество рекомендаций\n",
    "\n",
    "# Получаем рекомендации\n",
    "recommended_movies = get_movie_recommendations(deep_mf_model, user_id, movielens_train, n_recommendations, device='cpu')\n",
    "\n",
    "# Выводим рекомендации\n",
    "print(f\"Топ-{n_recommendations} рекомендаций для пользователя {user_id}:\")\n",
    "for movie in recommended_movies:\n",
    "    print(movie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусное изменение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class newDeepFM(nn.Module):\n",
    "    def __init__(self, tag_embedding_size=8, imdb_embedding_size=8, max_tags=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embeddings\n",
    "        self.user_embeddings = nn.Embedding(1001, 16)  # 1001 пользователей, 16 размерность\n",
    "        self.movie_embeddings = nn.Embedding(10020, 16)  # 10020 фильмов, 16 размерность\n",
    "        self.tag_embeddings = nn.Embedding(1000, tag_embedding_size, padding_idx=0)  # 1000 тегов, 8 размерность\n",
    "        self.imdb_embeddings = nn.Embedding(10000, imdb_embedding_size)  # 10000 возможных IMDb ID, 8 размерность\n",
    "\n",
    "        # Deep layers\n",
    "        input_size = 56  \n",
    "        self.deep_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        self.final_layer = nn.Linear(33, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Embeddings\n",
    "        user_emb = self.user_embeddings(batch['user']).view(batch['user'].size(0), -1)  # [batch_size, 16]\n",
    "        movie_emb = self.movie_embeddings(batch['movie']).view(batch['movie'].size(0), -1)  # [batch_size, 16]\n",
    "    \n",
    "        # Тег\n",
    "        tags = batch['tags'].clamp(0, self.tag_embeddings.num_embeddings - 1)  # Ограничиваем индексы\n",
    "        tag_emb = self.tag_embeddings(tags)  # [batch_size, max_tags, tag_embedding_size]\n",
    "        tag_emb = tag_emb.mean(dim=1)  # Усреднение по тегам, [batch_size, tag_embedding_size]\n",
    "\n",
    "        # IMDb\n",
    "        imdb_emb = self.imdb_embeddings(batch['imdb_info'].long()).view(batch['imdb_info'].size(0), -1)  # [batch_size, imdb_embedding_size]\n",
    "    \n",
    "        # FM Component\n",
    "        fm = (user_emb * movie_emb).sum(dim=1, keepdim=True)  # [batch_size, 1]\n",
    "    \n",
    "        # Deep Component\n",
    "        deep = torch.cat([user_emb, movie_emb, tag_emb, imdb_emb], dim=1)  # [batch_size, input_size]\n",
    "        deep = self.deep_layers(deep)  # [batch_size, 32]\n",
    "    \n",
    "        # Combine FM and Deep components\n",
    "        v = torch.cat([fm, deep], dim=1)  # [batch_size, 33]\n",
    "        v = self.final_layer(v)  # [batch_size, 1]\n",
    "    \n",
    "        # Output scaled to [0, 5]\n",
    "        return torch.sigmoid(v) * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "class MovielensDataset(Dataset):\n",
    "    def __init__(self, source, train=True, seed=1, max_tags=5):\n",
    "        # Загружаем данные\n",
    "        ratings = pd.read_csv(rf\"{source}\\ratings.csv\")\n",
    "        self.movies = pd.read_csv(rf\"{source}\\movies.csv\")\n",
    "        self.tags = pd.read_csv(rf\"{source}\\tags.csv\")\n",
    "        self.links = pd.read_csv(rf\"{source}\\links.csv\")\n",
    "        self.title_ratings = pd.read_csv(rf\"{source}\\title.ratings.tsv\", sep='\\t')  # Загружаем файл title.ratings\n",
    "\n",
    "        # Преобразуем ID фильмов в индексы\n",
    "        x = self.movies.loc[:, ['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "        self.links['movieId'] = self.links['movieId'].map(x.to_dict()['movieId'])\n",
    "        self.tags['movieId'] = self.tags['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # Словарь тегов для каждого фильма\n",
    "        self.movie_to_tags = self._preprocess_tags(max_tags)\n",
    "\n",
    "        # Разделение на обучающую и тестовую выборки\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data = ratings.drop(train_data.index)\n",
    "        self.ratings = train_data if train else test_data\n",
    "\n",
    "        # Создаем словарь для imdbId из title.ratings\n",
    "        self.imdb_ratings = self._create_imdb_ratings_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "\n",
    "        # Теги\n",
    "        tags = self.movie_to_tags.get(sample['movieId'], [0] * 5)  # Паддинг нулями\n",
    "\n",
    "        # Дополнительная информация по imdbId\n",
    "        imdb_info = self.get_imdb_info(sample['movieId'])\n",
    "\n",
    "        return {\n",
    "            \"user\": torch.LongTensor([sample['userId']]),\n",
    "            \"movie\": torch.LongTensor([sample['movieId']]),\n",
    "            \"rating\": torch.FloatTensor([sample['rating']]),\n",
    "            \"tags\": torch.LongTensor(tags),  # Теги как индексы\n",
    "            \"imdb_info\": torch.FloatTensor(imdb_info)  # Дополнительные данные по imdbId\n",
    "        }\n",
    "\n",
    "    def _preprocess_tags(self, max_tags):\n",
    "        \"\"\"Преобразование тегов в индексы.\"\"\"\n",
    "        unique_tags = list(set(self.tags['tag'].values))\n",
    "        tag_to_idx = {tag: idx + 1 for idx, tag in enumerate(unique_tags)}  # 0 для паддинга\n",
    "\n",
    "        # Формируем словарь {movieId: [tag_idx, ...]}\n",
    "        movie_to_tags = defaultdict(list)\n",
    "        for _, row in self.tags.iterrows():\n",
    "            movie_to_tags[row['movieId']].append(tag_to_idx[row['tag']])\n",
    "\n",
    "        # Ограничиваем количество тегов\n",
    "        for movie_id, tag_list in movie_to_tags.items():\n",
    "            movie_to_tags[movie_id] = tag_list[:max_tags] + [0] * (max_tags - len(tag_list))\n",
    "\n",
    "        return movie_to_tags\n",
    "\n",
    "    def _create_imdb_ratings_dict(self):\n",
    "        \"\"\"Создаем словарь {tconst: [averageRating, num_ratings]} из файла title.ratings.\"\"\"\n",
    "        # Переименовываем столбец для удобства работы с ним\n",
    "        self.title_ratings['tconst'] = self.title_ratings['tconst'].apply(lambda x: 'tt' + str(x)[2:])\n",
    "\n",
    "        imdb_ratings = self.title_ratings[['tconst', 'averageRating', 'numVotes']]\n",
    "        imdb_ratings_dict = imdb_ratings.set_index('tconst').to_dict('index')\n",
    "\n",
    "        return imdb_ratings_dict\n",
    "\n",
    "    def get_imdb_info(self, movie_id):\n",
    "        \"\"\"\n",
    "        Получает информацию о фильме через tconst (например, рейтинг IMDB) из загруженного файла title.ratings.\n",
    "        \"\"\"\n",
    "        # Извлекаем imdbId (tconst) из links.csv, добавляем 'tt', если его нет\n",
    "        imdb_id = self.links.loc[self.links['movieId'] == movie_id, 'imdbId'].values\n",
    "        if imdb_id:\n",
    "            imdb_id = str(imdb_id[0])  # Форматируем как строку для обращения в словарь\n",
    "            if not imdb_id.startswith('tt'):\n",
    "                imdb_id = 'tt' + imdb_id[2:]  # Добавляем префикс 'tt', если его нет\n",
    "\n",
    "            # Извлекаем данные из словаря\n",
    "            imdb_data = self.imdb_ratings.get(imdb_id, {'averageRating': 0.0, 'numVotes': 0})\n",
    "            imdb_rating = imdb_data['averageRating']\n",
    "            num_votes = imdb_data['numVotes']\n",
    "            \n",
    "            # Возвращаем нужную информацию (можно добавить больше данных, если необходимо)\n",
    "            return [imdb_rating, num_votes]\n",
    "        else:\n",
    "            return [0.0, 0]  # Если imdbId нет\n",
    "\n",
    "    def add_new_user(self, seed=1, num_movies=20):\n",
    "        \"\"\"Добавление нового пользователя с его оценками.\"\"\"\n",
    "        new_user_id = self.ratings['userId'].max() + 1\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        sampled_movie_ids = self.ratings['movieId'].unique()[:20]\n",
    "        sampled_ratings = np.round(np.random.uniform(1.0, 5.0, num_movies) * 2) / 2\n",
    "\n",
    "        new_user_ratings = pd.DataFrame({\n",
    "            'userId': new_user_id,\n",
    "            'movieId': sampled_movie_ids,\n",
    "            'rating': sampled_ratings,\n",
    "            'timestamp': int(pd.Timestamp.now().timestamp())\n",
    "        })\n",
    "\n",
    "        self.ratings = pd.concat([self.ratings, new_user_ratings], ignore_index=False)\n",
    "        return new_user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "sataset_source = r'.\\ml-latest-small'\n",
    "\n",
    "movielens_train = MovielensDataset(sataset_source, train=True)\n",
    "movielens_test  = MovielensDataset(sataset_source, train=False)\n",
    "\n",
    "train_loader = DataLoader(movielens_train, batch_size, True)\n",
    "test_loader = DataLoader(movielens_test, batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 2.538809  [  200/80669]\n",
      "loss: 1.085818  [20200/80669]\n",
      "loss: 1.169340  [40200/80669]\n",
      "loss: 1.155993  [60200/80669]\n",
      "loss: 0.999981  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 1.028674 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 0.972647  [  200/80669]\n",
      "loss: 1.158235  [20200/80669]\n",
      "loss: 1.071461  [40200/80669]\n",
      "loss: 1.122660  [60200/80669]\n",
      "loss: 0.990921  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.973830 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 0.967589  [  200/80669]\n",
      "loss: 1.062046  [20200/80669]\n",
      "loss: 1.154024  [40200/80669]\n",
      "loss: 0.796481  [60200/80669]\n",
      "loss: 1.141914  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.940766 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 1.101986  [  200/80669]\n",
      "loss: 1.093272  [20200/80669]\n",
      "loss: 0.905529  [40200/80669]\n",
      "loss: 0.887550  [60200/80669]\n",
      "loss: 0.984636  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.920440 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.851863  [  200/80669]\n",
      "loss: 0.912577  [20200/80669]\n",
      "loss: 0.865105  [40200/80669]\n",
      "loss: 0.971502  [60200/80669]\n",
      "loss: 0.830591  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.893889 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 0.844684  [  200/80669]\n",
      "loss: 0.803929  [20200/80669]\n",
      "loss: 0.817083  [40200/80669]\n",
      "loss: 0.771725  [60200/80669]\n",
      "loss: 0.882200  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.878997 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.891791  [  200/80669]\n",
      "loss: 0.932513  [20200/80669]\n",
      "loss: 0.946108  [40200/80669]\n",
      "loss: 0.779209  [60200/80669]\n",
      "loss: 0.953599  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.859541 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 0.939061  [  200/80669]\n",
      "loss: 0.661181  [20200/80669]\n",
      "loss: 0.834976  [40200/80669]\n",
      "loss: 0.810506  [60200/80669]\n",
      "loss: 0.920092  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.843384 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.812297  [  200/80669]\n",
      "loss: 0.844434  [20200/80669]\n",
      "loss: 0.854559  [40200/80669]\n",
      "loss: 0.838630  [60200/80669]\n",
      "loss: 0.796001  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.831951 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.680709  [  200/80669]\n",
      "loss: 0.662662  [20200/80669]\n",
      "loss: 0.738550  [40200/80669]\n",
      "loss: 0.662527  [60200/80669]\n",
      "loss: 0.661366  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.815000 \n",
      "\n",
      "== Epoch 11 ==\n",
      "loss: 0.940084  [  200/80669]\n",
      "loss: 0.631657  [20200/80669]\n",
      "loss: 0.731916  [40200/80669]\n",
      "loss: 0.804540  [60200/80669]\n",
      "loss: 0.707084  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.801948 \n",
      "\n",
      "== Epoch 12 ==\n",
      "loss: 0.813561  [  200/80669]\n",
      "loss: 0.620266  [20200/80669]\n",
      "loss: 0.768398  [40200/80669]\n",
      "loss: 0.641303  [60200/80669]\n",
      "loss: 0.745775  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.792455 \n",
      "\n",
      "== Epoch 13 ==\n",
      "loss: 0.634811  [  200/80669]\n",
      "loss: 0.692256  [20200/80669]\n",
      "loss: 0.637292  [40200/80669]\n",
      "loss: 0.712909  [60200/80669]\n",
      "loss: 0.951408  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.787887 \n",
      "\n",
      "== Epoch 14 ==\n",
      "loss: 0.763131  [  200/80669]\n",
      "loss: 0.710188  [20200/80669]\n",
      "loss: 0.676203  [40200/80669]\n",
      "loss: 0.809482  [60200/80669]\n",
      "loss: 0.815884  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.777380 \n",
      "\n",
      "== Epoch 15 ==\n",
      "loss: 0.724431  [  200/80669]\n",
      "loss: 0.597743  [20200/80669]\n",
      "loss: 0.668028  [40200/80669]\n",
      "loss: 0.626866  [60200/80669]\n",
      "loss: 0.726518  [80200/80669]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.774233 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "adeep_mf_model = newDeepFM().to(device)\n",
    "adeep_mf_loss = nn.MSELoss()\n",
    "adeep_mf_optimizer = torch.optim.Adam(adeep_mf_model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "ascheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(adeep_mf_optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "train(adeep_mf_model, train_loader, adeep_mf_loss, adeep_mf_optimizer, ascheduler, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10 рекомендаций для пользователя 611:\n",
      "Movie ID: 912, Title: Casablanca (1942), Predicted Rating: 4.51\n",
      "Movie ID: 177593, Title: Three Billboards Outside Ebbing, Missouri (2017), Predicted Rating: 4.51\n",
      "Movie ID: 858, Title: Godfather, The (1972), Predicted Rating: 4.49\n",
      "Movie ID: 2324, Title: Life Is Beautiful (La Vita è bella) (1997), Predicted Rating: 4.47\n",
      "Movie ID: 318, Title: Shawshank Redemption, The (1994), Predicted Rating: 4.47\n",
      "Movie ID: 1221, Title: Godfather: Part II, The (1974), Predicted Rating: 4.46\n",
      "Movie ID: 296, Title: Pulp Fiction (1994), Predicted Rating: 4.45\n",
      "Movie ID: 4973, Title: Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001), Predicted Rating: 4.45\n",
      "Movie ID: 44195, Title: Thank You for Smoking (2006), Predicted Rating: 4.45\n",
      "Movie ID: 1245, Title: Miller's Crossing (1990), Predicted Rating: 4.44\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_movie_recommendations(model, user_id, dataset, n_recommendations=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Получить рекомендации для конкретного пользователя.\n",
    "\n",
    "    :param model: Обученная модель\n",
    "    :param user_id: ID пользователя\n",
    "    :param dataset: Экземпляр датасета (например, MovielensDataset)\n",
    "    :param n_recommendations: Количество рекомендаций\n",
    "    :param device: Устройство ('cuda' или 'cpu')\n",
    "    :return: Список фильмов с рекомендованными рейтингами\n",
    "    \"\"\"\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    with torch.no_grad():\n",
    "        # Получаем список всех фильмов\n",
    "        all_movie_ids = dataset.movies['movieId'].values\n",
    "        \n",
    "        # Проверяем, какие фильмы пользователь уже оценил\n",
    "        rated_movies = dataset.ratings[dataset.ratings['userId'] == user_id]['movieId'].values\n",
    "        \n",
    "        # Фильтруем фильмы, которые пользователь еще не оценил\n",
    "        unseen_movies = [movie_id for movie_id in all_movie_ids if movie_id not in rated_movies]\n",
    "        \n",
    "        # Преобразуем индексы фильмов в индексы для эмбеддингов (если необходимо)\n",
    "        movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(all_movie_ids)}\n",
    "        unseen_movie_indices = [movie_to_idx[movie_id] for movie_id in unseen_movies]\n",
    "\n",
    "        # Получаем эмбеддинги для пользователя\n",
    "        user_tensor = torch.LongTensor([user_id] * len(unseen_movie_indices)).to(device)\n",
    "        movie_tensor = torch.LongTensor(unseen_movie_indices).to(device)\n",
    "        \n",
    "        # Получаем жанры фильмов (через one-hot encoding)\n",
    "        genres = dataset.movies.loc[dataset.movies['movieId'].isin(unseen_movies), 'genres'].fillna('')\n",
    "        all_genres = set(\" \".join(genres).split())  # Получаем все уникальные жанры\n",
    "\n",
    "        # Создаем one-hot encoding для жанров\n",
    "        genres_tensor = torch.zeros(len(unseen_movies), len(all_genres))  # One-hot encoding для жанров\n",
    "        for idx, genre_str in enumerate(genres):\n",
    "            genre_list = genre_str.split('|')\n",
    "            for genre in genre_list:\n",
    "                if genre in all_genres:  # Проверяем, что жанр есть в списке всех жанров\n",
    "                    genre_idx = list(all_genres).index(genre)\n",
    "                    genres_tensor[idx, genre_idx] = 1\n",
    "        genres_tensor = genres_tensor.to(device)\n",
    "\n",
    "        # Получаем теги (предположим, что они строковые)\n",
    "        tags_df = dataset.tags[dataset.tags['userId'] == user_id]\n",
    "        tags = tags_df[tags_df['movieId'].isin(unseen_movies)]['tag'].fillna('')\n",
    "\n",
    "        # Преобразуем теги в числовые представления (например, через one-hot encoding)\n",
    "        tag_to_idx = {tag: idx for idx, tag in enumerate(set(\" \".join(tags).split()))}\n",
    "        tags_tensor = torch.LongTensor([torch.tensor([tag_to_idx.get(tag, -1) for tag in movie_tags.split()]) \n",
    "                                       for movie_tags in tags]).to(device)\n",
    "\n",
    "        # Строим батч для вычисления предсказаний\n",
    "        batch = {\n",
    "            'user': user_tensor,\n",
    "            'movie': movie_tensor,\n",
    "            'genres': genres_tensor,\n",
    "            'tags': tags_tensor\n",
    "        }\n",
    "        \n",
    "        # Прогоняем через модель для получения рейтингов\n",
    "        predictions = model(batch).cpu().numpy()\n",
    "        \n",
    "        # Выбираем топ-N фильмов по предсказанным рейтингам\n",
    "        top_n_indices = np.argsort(predictions.flatten())[-n_recommendations:][::-1]\n",
    "        \n",
    "        # Создаем массив идентификаторов фильмов на основе полученных индексов\n",
    "        top_n_movie_ids = [unseen_movies[i] for i in top_n_indices]\n",
    "        top_n_ratings = predictions.flatten()[top_n_indices]\n",
    "\n",
    "        # Выводим рекомендации с названиями фильмов и предсказанными рейтингами\n",
    "        recommended_movies = []\n",
    "        for movie_id, rating in zip(top_n_movie_ids, top_n_ratings):\n",
    "            movie_title = dataset.movies.loc[dataset.movies['movieId'] == movie_id, 'title']\n",
    "            if movie_title.empty:\n",
    "                recommended_movies.append(f\"Movie ID: {movie_id} не найден в списке фильмов.\")\n",
    "            else:\n",
    "                movie_title = movie_title.values[0]\n",
    "                recommended_movies.append(f\"Movie ID: {movie_id}, Title: {movie_title}, Predicted Rating: {rating:.2f}\")\n",
    "        \n",
    "        return recommended_movies\n",
    "\n",
    "# Пример использования:\n",
    "user_id = 611  # ID пользователя, для которого генерируем рекомендации\n",
    "n_recommendations = 10  # Количество рекомендаций\n",
    "\n",
    "# Получаем рекомендации\n",
    "recommended_movies = get_movie_recommendations(deep_mf_model, user_id, movielens_train, n_recommendations, device='cpu')\n",
    "\n",
    "# Выводим рекомендации\n",
    "print(f\"Топ-{n_recommendations} рекомендаций для пользователя {user_id}:\")\n",
    "for movie in recommended_movies:\n",
    "    print(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
