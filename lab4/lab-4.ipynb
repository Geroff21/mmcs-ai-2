{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лаб-4. Рекомендации для коротких сессий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве датасета будем использовать архив из мудла.\n",
    "\n",
    "Это немного предобработанная версия [eCommerce events history](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-electronics-store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как и в предыдущей лабораторной пишем собственный загрузчик датасета\n",
    "class ECommerceDataset:\n",
    "    def __init__(self, path):\n",
    "        self.train_data = pd.read_csv(rf\"{path}/train_data.csv\")\n",
    "        self.test_data = pd.read_csv(rf\"{path}/test_data.csv\")\n",
    "\n",
    "        # Добавляем колонку с идентификаторами товаров (для эмбедингов)\n",
    "        all_data = pd.concat([self.train_data, self.test_data])\n",
    "        unique_items = all_data['product_id'].unique()\n",
    "        item_to_idx = pd.Series(data=np.arange(len(unique_items)), index=unique_items)\n",
    "        item_map = pd.DataFrame({'product_id': unique_items, 'product_index': item_to_idx[unique_items].values})\n",
    "        self.train_data = pd.merge(self.train_data, item_map, on='product_id', how='inner')\n",
    "        self.test_data  = pd.merge(self.test_data,  item_map, on='product_id', how='inner')\n",
    "\n",
    "        # Сортируем датасет так, чтобы все сессии оказались рядом, а клики внутри сессии упорядочились по времени\n",
    "        self.train_data.sort_values(['user_session', 'event_time'], inplace=True)\n",
    "        self.test_data.sort_values(['user_session', 'event_time'], inplace=True)\n",
    "\n",
    "# Загрузка большого датасета может занять некоторое время\n",
    "dataset = ECommerceDataset('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>product_id</th>\n",
       "      <th>user_session</th>\n",
       "      <th>product_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32678</th>\n",
       "      <td>1604329884</td>\n",
       "      <td>80548</td>\n",
       "      <td>003pEktS1X</td>\n",
       "      <td>4865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34407</th>\n",
       "      <td>1607580196</td>\n",
       "      <td>630753</td>\n",
       "      <td>00ImhDtWxv</td>\n",
       "      <td>4292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21963</th>\n",
       "      <td>1607165660</td>\n",
       "      <td>387956</td>\n",
       "      <td>00xjwy5Rb6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31665</th>\n",
       "      <td>1607168978</td>\n",
       "      <td>387956</td>\n",
       "      <td>00xjwy5Rb6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23220</th>\n",
       "      <td>1611391773</td>\n",
       "      <td>738</td>\n",
       "      <td>00zEpCxZUK</td>\n",
       "      <td>1478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14766</th>\n",
       "      <td>1613148682</td>\n",
       "      <td>93765</td>\n",
       "      <td>zzaAzAFcYL</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15086</th>\n",
       "      <td>1613148695</td>\n",
       "      <td>93765</td>\n",
       "      <td>zzaAzAFcYL</td>\n",
       "      <td>3193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32226</th>\n",
       "      <td>1613408761</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25067</th>\n",
       "      <td>1613409009</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>1613409044</td>\n",
       "      <td>564777</td>\n",
       "      <td>zzveLpjyyb</td>\n",
       "      <td>1226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41059 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       event_time  product_id user_session  product_index\n",
       "32678  1604329884       80548   003pEktS1X           4865\n",
       "34407  1607580196      630753   00ImhDtWxv           4292\n",
       "21963  1607165660      387956   00xjwy5Rb6              8\n",
       "31665  1607168978      387956   00xjwy5Rb6              8\n",
       "23220  1611391773         738   00zEpCxZUK           1478\n",
       "...           ...         ...          ...            ...\n",
       "14766  1613148682       93765   zzaAzAFcYL           3193\n",
       "15086  1613148695       93765   zzaAzAFcYL           3193\n",
       "32226  1613408761      564777   zzveLpjyyb           1226\n",
       "25067  1613409009      564777   zzveLpjyyb           1226\n",
       "2470   1613409044      564777   zzveLpjyyb           1226\n",
       "\n",
       "[41059 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных товаров 15316 = 15316\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Количество уникальных товаров',\n",
    "    pd.concat([dataset.train_data, dataset.test_data])['product_id'].nunique(),\n",
    "    '=',\n",
    "    pd.concat([dataset.train_data, dataset.test_data])['product_index'].max() + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За основу мы возьмём модель GRU4Rec из статьи\n",
    "\n",
    "https://arxiv.org/pdf/1511.06939\n",
    "\n",
    "Для обучения сети последовательными действиями пользователей, необходимо сконструировать минибатчи особым образом\n",
    "\n",
    "![Формирование минибатчей](images/mini-batch-creation.png)\n",
    "\n",
    "Так чтобы в инпуте батча содержался набор кликов пользователя, а в таргете набор следующих кликов из той же сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECommerceLoader():\n",
    "    def __init__(self, data, batch_size, shuffle=False):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.session_count = data['user_session'].nunique()\n",
    "\n",
    "        # Делаем массив с индексами начала и конца каждой сессии\n",
    "        session_sizes = np.array(data.groupby('user_session').size().cumsum())\n",
    "        self.offsets = np.append([0], session_sizes)\n",
    "\n",
    "    def __iter__(self):\n",
    "        session_order = np.arange(self.session_count)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(session_order)\n",
    "\n",
    "        # Заводим список активных сессий, размером с батч\n",
    "        active_sessions = np.arange(self.batch_size)\n",
    "        next_session = self.batch_size # индекс следующей сессии\n",
    "        start = self.offsets[session_order[active_sessions]]   # индексы начал активных сессий\n",
    "        end = self.offsets[session_order[active_sessions] + 1] # индексы концов активных сессий\n",
    "\n",
    "        closed_mask = list(active_sessions) # список сессий, которые открываются на текущей итерации\n",
    "        while True:\n",
    "            min_len = (end - start).min() # Количество итераций, которые мы можем пройти, пока не закончится какая-то сессия\n",
    "            idx_target = self.data['product_index'].values[start]\n",
    "\n",
    "            # Итерируем по сессиям до тех пор, пока какая-то не закончится\n",
    "            for i in range(min_len - 1):\n",
    "                idx_input = idx_target\n",
    "                idx_target = self.data['product_index'].values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input, target, closed_mask # маску мы будем использовать чтобы обнулять новые сессии\n",
    "                closed_mask = []\n",
    "\n",
    "            start = start + (min_len - 1)\n",
    "\n",
    "            # Пробегаемся по сессиям, которые должны быть завершены\n",
    "            closed_mask = np.arange(len(active_sessions))[(end - start) <= 1]\n",
    "            for idx in closed_mask:\n",
    "                # Если новых сессий нет, просто завершаемся\n",
    "                if next_session >= len(self.offsets) - 1:\n",
    "                    return\n",
    "                # Обновляем значения для новой сессии\n",
    "                active_sessions[idx] = next_session\n",
    "                start[idx] = self.offsets[session_order[next_session]]\n",
    "                end[idx]   = self.offsets[session_order[next_session] + 1]\n",
    "                next_session += 1\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader = ECommerceLoader(dataset.train_data, batch_size, shuffle=True)\n",
    "test_loader  = ECommerceLoader(dataset.test_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама модель основана на рекуррентной архитектуре, такие сети помимо выходного состояния возвращают ещё и скрытое состояние, которое передаётся в сеть на следующей итерации, позволяя таким образом обрабатывать связанные последовательности данных (такие как текст или действия пользователя).\n",
    "\n",
    "![рекуррентная сеть](images/Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "В данном случае используется архитектура GRU (это как LSTM, но ещё лучше)\n",
    "![](images/lstm-gru.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU4Rec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        embedding_size = 64\n",
    "        hidden_size = 64\n",
    "        item_size = 15316\n",
    "\n",
    "        # будем хранить внутреннее состояние внутри модели\n",
    "        self.state = torch.zeros([batch_size, hidden_size])\n",
    "\n",
    "        self.embedding = nn.Embedding(item_size, embedding_size)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.gru = nn.GRUCell(embedding_size, hidden_size)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, item_size)\n",
    "        )\n",
    "\n",
    "    # Перегрузка to чтобы состояние тоже перевести на девайс\n",
    "    def to(self, device):\n",
    "        self.state = self.state.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    # Обнуляем состояние для новых сессий\n",
    "    def update_state(self, mask):\n",
    "        self.state = self.state.detach()\n",
    "        self.state[mask, :] = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        v = input.unsqueeze(1)\n",
    "        v = self.embedding(v)\n",
    "        v = self.flatten(v)\n",
    "        v = self.state = self.gru(v, self.state)\n",
    "        v = self.output_layer(v)\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тренировка происходит и тестирование\n",
    "\n",
    "def train_iteration(model, data_loader, loss_function, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch, (x, y, m) in enumerate(data_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Не забываем обнулить состояние\n",
    "        model.update_state(m)\n",
    "\n",
    "        pred = model(x)\n",
    "        loss = loss_function(pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(x)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}]\")\n",
    "\n",
    "def test(model, data_loader, loss_function):\n",
    "    model.eval()\n",
    "\n",
    "    loss, correct, count = 0, 0 ,0\n",
    "    with torch.no_grad():\n",
    "        for x, y, m in data_loader:\n",
    "            count += 1\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model.update_state(m)\n",
    "            pred = model(x)\n",
    "            loss += loss_function(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss = loss / count\n",
    "    correct /= count * batch_size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(epochs, model, loss_function, optimizer):\n",
    "    for t in range(epochs):\n",
    "        print(f\"== Epoch {t + 1} ==\")\n",
    "        train_iteration(model, train_loader, loss_function, optimizer)\n",
    "        test(model, test_loader, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 9.744731  [   10]\n",
      "loss: 4.967004  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 10.454657 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 4.320260  [   10]\n",
      "loss: 3.804390  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 11.685538 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 2.994867  [   10]\n",
      "loss: 1.965275  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 12.547649 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 1.672259  [   10]\n",
      "loss: 2.283740  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 13.124784 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 1.444261  [   10]\n",
      "loss: 1.136153  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 13.688651 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 0.677080  [   10]\n",
      "loss: 1.550114  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 14.254824 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 0.756532  [   10]\n",
      "loss: 0.975992  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.1%, Avg loss: 14.753973 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 1.198874  [   10]\n",
      "loss: 1.745914  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 15.245403 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 0.631831  [   10]\n",
      "loss: 0.805795  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 15.797685 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 0.762691  [   10]\n",
      "loss: 1.336677  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 16.193495 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GRU4Rec().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(10, model, loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задания\n",
    "\n",
    "Основные:\n",
    "- Достичь точности в 3.5% на этом датасете - 5 баллов\n",
    "- На основе GRU4Rec построить модель для датасета из предыдущей лабораторной (Movielens) - 5 баллов\n",
    "\n",
    "Дополнительные задания:\n",
    "- Реализовать одну из функций потерь BPR или TOP1 (https://arxiv.org/pdf/1511.06939) - 5 баллов\n",
    "- Реализовать вторую функцию потерь - 5 баллов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки\n",
    "\n",
    "Полезные ссылки по рекомендательным системам, модели из лекции и не только\n",
    "\n",
    "- Репозиторий с кучей информации по рекомендательным системам https://github.com/recommenders-team/recommenders\n",
    "- Рекомендательные системы на основе свёрток https://arxiv.org/pdf/1809.07426\n",
    "- Sequence-Aware Factorization Machines (машина факторизации для временных последовательностей) https://arxiv.org/pdf/1911.02752\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Epoch 1 ==\n",
      "loss: 9.655902  [   10]\n",
      "loss: 5.624956  [10010]\n",
      "Test Error: \n",
      " Accuracy: 2.9%, Avg loss: 10.369296 \n",
      "\n",
      "== Epoch 2 ==\n",
      "loss: 5.557940  [   10]\n",
      "loss: 5.096666  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.6%, Avg loss: 11.750174 \n",
      "\n",
      "== Epoch 3 ==\n",
      "loss: 2.638438  [   10]\n",
      "loss: 2.859302  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 12.504964 \n",
      "\n",
      "== Epoch 4 ==\n",
      "loss: 1.915605  [   10]\n",
      "loss: 2.566915  [10010]\n",
      "Test Error: \n",
      " Accuracy: 3.9%, Avg loss: 13.082052 \n",
      "\n",
      "== Epoch 5 ==\n",
      "loss: 0.931095  [   10]\n",
      "loss: 2.579995  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 13.631600 \n",
      "\n",
      "== Epoch 6 ==\n",
      "loss: 2.264124  [   10]\n",
      "loss: 1.249820  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.1%, Avg loss: 14.255540 \n",
      "\n",
      "== Epoch 7 ==\n",
      "loss: 1.345037  [   10]\n",
      "loss: 1.363195  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 14.758796 \n",
      "\n",
      "== Epoch 8 ==\n",
      "loss: 1.359853  [   10]\n",
      "loss: 0.749213  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 15.230775 \n",
      "\n",
      "== Epoch 9 ==\n",
      "loss: 1.257559  [   10]\n",
      "loss: 1.216691  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 15.738139 \n",
      "\n",
      "== Epoch 10 ==\n",
      "loss: 1.078325  [   10]\n",
      "loss: 0.514311  [10010]\n",
      "Test Error: \n",
      " Accuracy: 4.0%, Avg loss: 16.225894 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GRU4Rec().to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(10, model, loss, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class MovielensDataset(Dataset):\n",
    "    r\"\"\"seed должен быть одинаковым для обучающей и тренировочной выборки\"\"\"\n",
    "    def __init__(self, source, train=True, seed=1, max_seq_len=50):\n",
    "        ratings = pd.read_csv(rf\"{source}\\ratings.csv\")\n",
    "        self.movies = pd.read_csv(rf\"{source}\\movies.csv\")\n",
    "\n",
    "        # Преобразовываем Id фильмов в индексы в таблице movies\n",
    "        x = self.movies.loc[:,['movieId']]\n",
    "        x['movieId'], x.index = x.index, x['movieId'].values\n",
    "        ratings['movieId'] = ratings['movieId'].map(x.to_dict()['movieId'])\n",
    "\n",
    "        # делим датасет 80% на 20%\n",
    "        train_data = ratings.sample(frac=0.8, random_state=seed)\n",
    "        test_data  = ratings.drop(train_data.index)\n",
    "\n",
    "        self.ratings = train_data if train else test_data\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ratings.iloc[idx]\n",
    "        user_id = sample['userId']\n",
    "        movie_id = sample['movieId']\n",
    "        rating = sample['rating']\n",
    "        \n",
    "        # Создаем последовательность для каждого пользователя\n",
    "        user_ratings = self.ratings[self.ratings['userId'] == user_id]\n",
    "        sequence = user_ratings.sort_values(by='timestamp')['movieId'].values\n",
    "\n",
    "        # Обрезаем или дополняем последовательность до max_seq_len\n",
    "        sequence = sequence[-self.max_seq_len:]  # Оставляем последние max_seq_len фильмов\n",
    "        sequence = torch.LongTensor(sequence)\n",
    "        \n",
    "        # Добавляем паддинг (если последовательность меньше max_seq_len)\n",
    "        if len(sequence) < self.max_seq_len:\n",
    "            sequence = torch.cat([torch.zeros(self.max_seq_len - len(sequence), dtype=torch.long), sequence])\n",
    "        \n",
    "        return {\n",
    "            \"input_seq\": sequence,\n",
    "            \"target_movie\": torch.LongTensor([movie_id]),\n",
    "            \"rating\": torch.FloatTensor([rating])\n",
    "        }\n",
    "\n",
    "    def add_new_user(self, seed=1, num_movies=20):\n",
    "        \"\"\"Метод для добавления нового пользователя с его оценками\"\"\"\n",
    "        new_user_id = self.ratings['userId'].max() + 1\n",
    "        np.random.seed(seed)\n",
    "        sampled_movie_ids = self.ratings['movieId'].unique()[:20]\n",
    "        sampled_ratings = np.round(np.random.uniform(1.0, 5.0, num_movies) * 2) / 2\n",
    "    \n",
    "        # Создание данных для нового пользователя\n",
    "        new_user_ratings = pd.DataFrame({\n",
    "            'userId': new_user_id,\n",
    "            'movieId': sampled_movie_ids,\n",
    "            'rating': sampled_ratings,\n",
    "            'timestamp': int(pd.Timestamp.now().timestamp())\n",
    "        })\n",
    "    \n",
    "        # Добавление нового пользователя в рейтинг\n",
    "        self.ratings = pd.concat([self.ratings, new_user_ratings], ignore_index=False)\n",
    "        return new_user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRU4RecMovielens(nn.Module):\n",
    "    def __init__(self, num_movies, embedding_size=64, hidden_size=128, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_movies = num_movies\n",
    "\n",
    "        # Эмбеддинги для фильмов\n",
    "        self.embedding = nn.Embedding(num_movies, embedding_size)\n",
    "\n",
    "        # GRU для обработки последовательностей\n",
    "        self.gru = nn.GRU(\n",
    "            embedding_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Финальный линейный слой для предсказания\n",
    "        self.output_layer = nn.Linear(hidden_size, num_movies)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        input_seq: тензор с индексами фильмов [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # Эмбеддинги\n",
    "        embedded = self.embedding(input_seq)  # [batch_size, seq_len, embedding_size]\n",
    "\n",
    "        # GRU\n",
    "        output, _ = self.gru(embedded)\n",
    "\n",
    "        # Берем последнее состояние\n",
    "        final_state = output[:, -1, :]  # [batch_size, hidden_size]\n",
    "\n",
    "        # Предсказания\n",
    "        logits = self.output_layer(final_state)  # [batch_size, num_movies]\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 8.238102766314654\n",
      "Epoch 2/10, Loss: 7.669535947668794\n",
      "Epoch 3/10, Loss: 7.324535045487594\n",
      "Epoch 4/10, Loss: 7.065345988298209\n",
      "Epoch 5/10, Loss: 6.887523855400766\n",
      "Epoch 6/10, Loss: 6.76639290813036\n",
      "Epoch 7/10, Loss: 6.680563179948225\n",
      "Epoch 8/10, Loss: 6.614524036303062\n",
      "Epoch 9/10, Loss: 6.565735125059365\n",
      "Epoch 10/10, Loss: 6.526440516581189\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Инициализация параметров\n",
    "batch_size = 32\n",
    "embedding_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "max_seq_len = 50\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = MovielensDataset(source=\"../lab3/ml-latest-small\", train=True)\n",
    "test_dataset = MovielensDataset(source=\"../lab3/ml-latest-small\", train=False)\n",
    "\n",
    "num_movies = len(train_dataset.movies['movieId'].unique())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Инициализация модели\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRU4RecMovielens(num_movies=num_movies, embedding_size=embedding_size,\n",
    "                         hidden_size=hidden_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "# Оптимизатор и функция потерь\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_seq = batch[\"input_seq\"].to(device)\n",
    "        target_movie = batch[\"target_movie\"].to(device)\n",
    "        \n",
    "        # Прямой проход\n",
    "        predictions = model(input_seq)\n",
    "        loss = loss_function(predictions, target_movie.squeeze())\n",
    "        \n",
    "        # Обратный проход\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
